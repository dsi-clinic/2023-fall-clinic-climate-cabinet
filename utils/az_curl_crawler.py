import pandas as pd
import requests


def az_wrapper(
    page: str, start_year=2023, end_year=2023, *args, **kwargs: int
) -> pd.DataFrame:
    """Take desired page, start and end years, and return the scraped data

    This function retrieves and compiles the data from a given table
    from the arizona database, whether aggregate or detailed,
    within the given time period
    NOTE: Empty returns are to be expected for some inputs,
    as some tables are empty or near empty even for long spans of time.

    Args: page: the name of a basic or detailed page in the
    Arizona dataset, excluding the Name page.
    start_year: earliest year to include scraped data, inclusive
    end_year: last year to include scraped data, inclusive

    Returns: a pandas dataframe of the
    """

    page = pages_dict[page]

    if page < 10:
        return scrape_wrapper(page, start_year, end_year)

    else:
        det = detailed_wrapper_director(page)
        agg_df = scrape_wrapper(det, start_year, end_year)
        entities = entities = agg_df["EntityID"]

        return detailed_scrape_wrapper(entities, page, start_year, end_year)


def detailed_wrapper_director(page: int) -> int:
    """Turn detailed page number into parent page number

    This function takes as input the number of the page input to
    scrape_wrapper in the course of az_wrapper, and derives the
    number of the parent page which must be scraped for
    entities first

    Args: page: the two-digit page number belonging to a
    detailed page as shown in pages_dict.

    Returns: an integer representing the parent page
    """

    if page in [20, 21, 22, 23, 24]:
        return 1

    elif page in [30, 31, 32, 33, 34, 35, 36]:
        return 2

    elif page in [40, 41, 42]:
        return 3

    elif page in [50, 51, 52, 53, 54]:
        return 4

    elif page in [60, 61, 62]:
        return 5

    elif page in [70, 71, 72]:
        return 6

    elif page in [80]:
        return 7

    elif page in [90]:
        return 8


def scrape_wrapper(page, start_year, end_year, *args: int) -> pd.DataFrame:
    """Create parameters and scrape an aggregate table

    This function is called by az_wrapper() to create the parameters and
    call the basic scraper for a certain basic page. To scrape the detailed
    pages, use detailed_scrape_wrapper() instead.

    Args: page: the one-digit number representing one of the eight
    basic pages in the arizona dataset, such as Candidates, PAC,
    Individual Contributions, etc. Refer to pages_dict
    start_year: earliest year to include scraped data, inclusive
    end_year: last year to include scraped data, inclusive

    Returns: a pandas dataframe containing the table data for
    the selected timeframe
    """
    params = parametrize(page, start_year, end_year)
    res = scrape(params, head, base_data)
    results = res.json()
    df = pd.DataFrame(data=results["data"])
    df = df.reset_index().drop(columns={"index"})
    return df


def detailed_scrape_wrapper(
    entities: pd.core.series.Series, page: int, start_year: int, end_year: int
) -> pd.DataFrame:
    """Create parameters and scrape an aggregate table

    This function is called by az_wrapper() to create the parameters and
    call the detailed scraper for a certain detailed page. To scrape the
    basic pages, use scrape_wrapper() instead.

    Args: page: the two-digit number representing a sub-page of
    one of the eight basic pages, such as Candidates/Income,
    PAC/All Transactions, etc. Refer to pages_dict
    start_year: earliest year to include scraped data, inclusive
    end_year: last year to include scraped data, inclusive

    Returns: a pandas dataframe containing the table data for
    the selected timeframe
    """

    d_params = []

    for entity in entities:
        ent = detailed_parametrize(entity, page, start_year, end_year)
        d_params.append(ent)

    dfs = []

    for d_param in d_params:
        res = detailed_scrape(d_param, start_year, end_year)
        results = res.json()
        dfs.append(pd.DataFrame(data=results["data"]))

    return pd.concat(dfs).reset_index().drop(columns={"index"})


def scrape(params: dict, headers: dict, data: dict) -> requests.models.Response:
    """Scrape a table from the main arizona site

    This function takes in the header and base provided
    elsewhere, and parameters generated by parametrize(),
    to locate and scrape data from one of the eight
    aggregate tables on the Arizona database.

    Args: params: created from parametrize(), containing
    the page, start and end years, table page, and table length.
    Note that 'page' encodes the page to be scraped, such as
    Candidates, IndividualContributions, etc. Refer to the
    attached Pages dictionary for details.
    headers: necessary for calling the response, provided above
    data: necessary for calling the response, provided above
    """

    return requests.post(
        "https://seethemoney.az.gov/Reporting/GetNEWTableData/",
        params=params,
        # cookies=cookies,
        headers=head,
        data=base_data,
    )


def detailed_scrape(
    detailed_params: dict, headers: dict, data: dict
) -> requests.models.Response:
    """Scrape a sub-table from the arizona database

    This function takes an entity number, which can be
    gathered from an aggregate table using scrape() or
    inputted manually, and gathers that entity's detailed
    information within the specified time frame from
    one of the sub-tables.

    Args: detailed_params: created from detailed_parametrize(),
    containing the entity_id, page, start and end years, table page,
    and table length.
    Note that 'page' encodes the page to be scraped, such as
    Candidates, IndividualContributions, etc. Refer to the
    attached Pages dictionary for details.
    headers: necessary for calling the response, provided above
    data: necessary for calling the response, provided above
    """

    return requests.post(
        "https://seethemoney.az.gov/Reporting/GetNEWDetailedTableData/",
        params=detailed_params,
        # cookies=cookies,
        headers=head,
        data=base_data,
    )


def parametrize(
    page=1,
    start_year=2023,
    end_year=2025,
    table_page=1,
    table_length=500000,
    **kwargs: int
) -> dict:
    """Input parameters for scrape and return as dict

    This function takes in parameters to scrape a
    given section of the arizona database, and turns
    them into a dictionary to be fed into scrape() as params

    Kwargs: page: encodes the page to be scraped, such as
    Candidates, Individual Contributions, etc. Refer to the
    attached Pages dictionary for details.
    start_year: earliest year to include scraped data, inclusive
    end_year: last year to include scraped data, inclusive
    table_page: the numbered page to be accessed. Only necessary
    to iterate on this if accessing large quantities of Individual
    Contributions data, as all other data will be captured whole by
    the default table_length
    table_length: the length of the table to be scraped. The default
    setting should scrape the entirety of the desired data unless
    looking at Individual Contributions

    Returns: a dictionary of the parameters, to be fed into scrape()
    """
    return {
        "Page": str(page),  # refers to the overall page, like candidates
        # or individual expenditures
        "startYear": str(start_year),
        "endYear": str(end_year),
        "JurisdictionId": "0|Page",  # we keep this in here,
        # but don't use it, not sure what it does?
        "TablePage": str(table_page),
        "TableLength": str(table_length),
        "ChartName": str(page),
        "IsLessActive": "false",  # have yet to experiment with these
        "ShowOfficeHolder": "false",  # have yet to experiment with these
    }


def detailed_parametrize(
    entity_id,
    page=1,
    start_year=2023,
    end_year=2025,
    table_page=1,
    table_length=500000,
    *args: int,
    **kwargs: int
) -> dict:
    """ """
    return {
        "Page": str(page),  # refers to the overall page, like candidates
        # or individual expenditures
        "startYear": str(start_year),
        "endYear": str(end_year),
        "JurisdictionId": "0|Page",  # we keep this in here,
        # but don't use it, not sure what it does?
        "TablePage": str(table_page),
        "TableLength": str(table_length),
        "Name": "1~" + str(entity_id),  # these two get used
        # when scraping detailed data
        "entityId": str(entity_id),
        "ChartName": str(page),
        "IsLessActive": "false",  # have yet to experiment with these
        "ShowOfficeHolder": "false",  # have yet to experiment with these
    }


pages_dict = {
    "Candidate": 1,
    "PAC": 2,
    "Political Party": 3,
    "Organzations": 4,
    "Independent Expenditures": 5,
    "Ballot Measures": 6,
    "Individual Contributors": 7,
    "Vendors": 8,
    "Name": 11,
    "Candidate/Income": 20,
    "Candidate/Expense": 21,
    "Candidate/IEFor": 22,
    "Candidate/IEAgainst": 23,
    "Candidate/All Transactions": 24,
    "PAC/Income": 30,
    "PAC/Expense": 31,
    "PAC/IEFor": 32,
    "PAC/IEAgainst": 33,
    "PAC/BMEFor": 34,
    "PAC/BMEAgainst": 35,
    "PAC/All Transactions": 36,
    "Political Party/Income": 40,
    "Political Party/Expense": 41,
    "Political Party/All Transactions": 42,
    "Organizations/IEFor": 50,
    "Organizations/IEAgainst": 51,
    "Organizations/BMEFor": 52,
    "Organizations/BME Against": 53,
    "Organizations/All Transactions": 54,
    "Independent Expenditures/IEFor": 60,
    "Independent Expenditures/IEAgainst": 61,
    "Independent Expenditures/All Transactions": 62,
    "Ballot Measures/Amount For": 70,
    "Ballot Measures/Amount Against": 71,
    "Ballot Measures/All Transactions": 72,
    "Individuals/All Transactions": 80,
    "Vendors/All Transactions": 90,
}

head = {
    "authority": "seethemoney.az.gov",
    "accept": "application/json, text/javascript, */*; q=0.01",
    "accept-language": "en-US,en;q=0.7",
    "content-type": "application/x-www-form-urlencoded; charset=UTF-8",
    "origin": "https://seethemoney.az.gov",
    "referer": "https://seethemoney.az.gov/Reporting/Explore",
    "sec-ch-ua": '"Chromium";v="116", "Not)A;Brand";v="24", "Brave";v="116"',
    "sec-ch-ua-mobile": "?0",
    "sec-ch-ua-platform": '"macOS"',
    "sec-fetch-dest": "empty",
    "sec-fetch-mode": "cors",
    "sec-fetch-site": "same-origin",
    "sec-gpc": "1",
    """user-agent""": """Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)
    AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.0.0 Safari/537.36""",
    "x-requested-with": "XMLHttpRequest",
}

base_data = {
    "draw": "2",
    "order[0][column]": "0",
    "order[0][dir]": "asc",
    "start": "0",
    "length": "500000",
    "search[value]": "",
    "search[regex]": "false",
}
